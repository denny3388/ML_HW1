# ML HW1
> 0716081 è‘‰æ™¨

## Project preview
1. Data Input
2. Data Visualization
3. Data Preprocessing
4. Model Construction
5. Train-Test-Split
5. Results
6. Compare and Conclusion
7. Question

## Data Input
### Mushroom
```python
name_list = ['edible','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing','gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring','stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population','habitat']
feature_list = name_list
feature_list.remove('edible')
attribute_list = [['b','c','x','f','k','s'],['f','g','y','s'],['n','b','c','g','r','p','u','e','w','y'],['t','f'],['a','l','c','y','f','m','n','p','s'],['a','d','f','n'],['c','w','d'],['b','n'],['k','n','b','h','g','r','o','p','u','e','w','y'],['e','t'],['b','c','u','e','z','r'],['f','y','k','s'],['f','y','k','s'],['n','b','c','g','o','p','e','w','y'],['n','b','c','g','o','p','e','w','y'],['p','u'],['n','o','w','y'],['n','o','t'],['c','e','f','l','n','p','s','z'],['k','n','b','h','r','o','u','w','y'],['a','c','n','s','v','y'],['g','l','m','p','u','w','d']]
cate_num_list = [6,4,10,2,9,4,3,2,12,2,4,4,9,9,2,4,3,8,9,6,7]
data = pd.read_csv('agaricus-lepiota.data', sep=",", names=name_list)
```

### Iris
```python
name_list = ['sepal-length','sepal-width','petal-length','petal-width','class']
feature_list = name_list
feature_list.remove('class')
label_list = ['Iris-setosa','Iris-versicolor','Iris-virginica']
data = pd.read_csv('iris.data', sep=",", names=name_list)
data_group = data.groupby('class')

```
#### Variables
- **name_list**: ç‚º .data æª”æ¡ˆåŠ ä¸Šæ¯å€‹ column çš„èªªæ˜
- **feature_list**: æ¯å€‹ feature çš„åç¨±
- **attribute_list**: æ¯å€‹ feature çš„æ‰€æœ‰å¯èƒ½çš„å€¼ (Mushroom)
- **cate_num_list**: æ¯å€‹ feature çš„æ‰€æœ‰å¯èƒ½çš„å€¼çš„**æ•¸é‡** (Mushroom)
- **label_list**: label çš„æ‰€æœ‰å¯èƒ½çš„å€¼ (Iris)

## Data Visualization (æœªå®Œæˆï¼ï¼ï¼)
### Mushroom

## Data Preprocessing
### Mushroom
```python
# Drop feature w/ missing value 
data = data.drop(columns='stalk-root')
feature_list.remove('stalk-root')

# shuffle
data = data.sample(frac=1)
```
- Drop features with missing value
- Shuffle the data using `sameple()`

### Iris
```python
# Divide data into group according to their class
data_group = data.groupby('class')

# shuffle
data = data.sample(frac=1)
```
- å°‡ data ä»¥ä»–å€‘çš„ class åˆ†çµ„ï¼Œä¹‹å¾Œåœ¨åšè³‡æ–™åˆ†ææ™‚è¼ƒæ–¹ä¾¿
- Shuffle the data using `sameple()`

## Model Construction
### Mushroom

æˆ‘ä½¿ç”¨äº†`CategoricalNB()`ç•¶ä½œmodelï¼Œä¸¦ä¸”å°‡**Model Construction**ã€**Validation**å¯«åœ¨å‡½å¼`holdout()`åŠ`kfold()`è£¡é¢

```python
model = CategoricalNB(alpha=alpha_, min_categories=cate_num_list)
```

### Iris

æˆ‘ä½¿ç”¨äº†`GaussianNB()`ç•¶ä½œmodelï¼Œä¸¦ä¸”å°‡**Model Construction**ã€**Validation**å¯«åœ¨å‡½å¼`holdout()`åŠ`kfold()`è£¡é¢

```python
model = GaussianNB()
```

## Train-Test-Split
### Mushroom
#### Select data for modeling
```python
X=data[feature_list]
y=data['edible'].values

enc = OrdinalEncoder()
X = enc.fit_transform(X)
```
- **X**: features
- **y**: label (edible)

è¦ç”¨ OrdinalEncoder() æ˜¯å› ç‚ºè¦æŠŠ feature çš„ char è½‰æˆ int å‹æ…‹

#### Holdout
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
clf = model.fit(X_train, y_train)
```
åˆ©ç”¨ `train_test_split()` å°‡ X, y åˆ†æˆ **Train:Test = 7:3**

ä¹‹å¾Œå°‡ training data åˆ©ç”¨ `fit()` é¤µå…¥å»ºç«‹å¥½çš„ model

#### K-fold
```python
K = 3
kf = KFold(n_splits=K)

for train_index, test_index in kf.split(X,y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    clf = model.fit(X_train, y_train)
```
åˆ©ç”¨ `Kfold()` å°‡ X, y åˆ†æˆæŒ‡å®šå€é–“

ä¹‹å¾Œåˆ©ç”¨ for loop åŠ ä¸Š `Kfold.split()` é€æ¬¡å°‡æ¯å€‹ fold çš„ training data é¤µå…¥ model

### Iris

#### Select data for modeling
```python
X=data[feature_list]
y=data['class'].values
```
- **X**: features
- **y**: label (class)

#### Holdout
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
clf = model.fit(X_train, y_train)
```
åˆ©ç”¨ `train_test_split()` å°‡ X, y åˆ†æˆ **Train:Test = 7:3**

ä¹‹å¾Œå°‡ training data åˆ©ç”¨ `fit()` é¤µå…¥å»ºç«‹å¥½çš„ model

#### K-fold
```python
K = 3
kf = KFold(n_splits=K)

for train_index, test_index in kf.split(X,y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    clf = model.fit(X_train, y_train)
```
åˆ©ç”¨ `Kfold()` å°‡ X, y åˆ†æˆæŒ‡å®šå€é–“

ä¹‹å¾Œåˆ©ç”¨ for loop åŠ ä¸Š `Kfold.split()` é€æ¬¡å°‡æ¯å€‹ fold çš„ training data é¤µå…¥ model

## Results
### Mushroom

æ­¤è™•ä»¥ **'e' ç‚º positive**ï¼Œä¸¦è¨ˆç®— Confusion matrix, Accuracy, Sensitivity(Recall), Precision ç­‰æ•¸å€¼
```python
TP, FP, FN, TN = 0, 0, 0, 0
pred = clf.predict(X_test)
for i in range(len(X_test)):
    if pred[i] == 'e' and y_test[i] == 'e':
        TP += 1
    elif pred[i] == 'e' and y_test[i] == 'p':
        FP += 1
    elif pred[i] == 'p' and y_test[i] == 'e':
        FN += 1
    elif pred[i] == 'p' and y_test[i] == 'p':
        TN += 1
confusion_mat = pd.DataFrame([[TP, FP], [FN, TN]])
acc = (TP+TN)/len(X_test)
recall = TP/(TP+FN)
precision = TP/(TP+FP)
```

#### Confusion matrix
- Without Laplace

|Validation:|Holdout|   | K-fold |    |
|  ----  | ----  | ---- | ---- | ---- |
|**Predict \ Actual**|**e** |**p**|**e**|**p**|
|**e**| 1256 | 2         |1395.67|1    |
|**p**| 9    |1171       |7    |1304.33|
> K-fold çš„ confusion matrix ç‚ºä¸‰å€‹ fold è¨ˆç®—çµæœç›¸åŠ ä¹‹å¾Œå†å–å¹³å‡

- With Laplace

|Validation:|Holdout|   | K-fold |    |
|  ----  | ----  | ---- | ---- | ---- |
|**Predict \ Actual**|**e** |**p**|**e**|**p**|
|**e**| 1249 | 104       |1395|104.33    |
|**p**| 8    | 1077      |7.67 |1201|

#### Accuracy, Sensitivity(Recall), Precision

|Validation:| \|-----| Holdout  | -----\||\|-----|  K-fold  |-----\||
|  ----  | ----  | ---- | ---- | ---- | ---- | ---- |
|**Smoothing \ Performance(%)**|**Acc** |**Sens**|**Prec**|**Acc** |**Sens**|**Prec**|
|**Without Laplace**|99.63|99.36|99.92|99.7 |99.51|99.93|
|**With Laplace**   |95.41|99.36|92.31|95.86|99.46|93.04|


### Iris

æ­¤è™• **åˆ†åˆ¥ä»¥ä¸‰å€‹ä¸åŒ class ç‚º positive**ï¼Œä¸¦è¨ˆç®— Confusion matrix, Accuracy, Sensitivity(Recall), Precision ç­‰æ•¸å€¼
```python
for p in range(3): # for each class being positive
    TP, FP, FN, TN = 0, 0, 0, 0
    pred = clf.predict(X_test)
    positive = label_list[p]
    for i in range(len(X_test)):
        if pred[i] == positive and y_test[i] == positive:
            TP += 1
        elif pred[i] == positive and y_test[i] != positive:
            FP += 1
        elif pred[i] != positive and y_test[i] == positive:
            FN += 1
        elif pred[i] != positive and y_test[i] != positive:
            TN += 1
    mat = pd.DataFrame([[TP, FP], [FN, TN]])
    confusion_mat.append(mat)
    acc.append((TP+TN)/len(X_test))
    recall.append(TP/(TP+FN))
    precision.append(TP/(TP+FP))
```

#### Confusion matrix
- Positive = **Iris-setosa**

|Validation:|Holdout|   | K-fold |    |
|  ----  | ----  | ---- | ---- | ---- |
|**Predict \ Actual**|**Iris-setosa** |**Else**|**Iris-setosa**|**Else**|
|**Iris-setosa**| 19 | 0         |16.67|0    |
|**Else**| 0    |26       |0    |33.33|

- Positive = **Iris-versicolor**

|Validation:|Holdout|   | K-fold |    |
|  ----  | ----  | ---- | ---- | ---- |
|**Predict \ Actual**|**Iris-versicolor** |**Else**|**Iris-versicolor**|**Else**|
|**Iris-versicolor**| 13 | 7         |15.67|1.33    |
|**Else**| 0    |31       |1    |32|

- Positive = **Iris-virginica**

|Validation:|Holdout|   | K-fold |    |
|  ----  | ----  | ---- | ---- | ---- |
|**Predict \ Actual**|**Iris-virginica** |**Else**|**Iris-virginica**|**Else**|
|**Iris-virginica**|12|0	|15.33|	1|
|**Else**|1	|32|1.33|	32.33|

#### Accuracy, Sensitivity(Recall), Precision

|Validation:| \|-----| Holdout  | -----\||\|-----|  K-fold  |-----\||
|  ----  | ----  | ---- | ---- | ---- | ---- | ---- |
|**Positive \ Performance(%)**|**Acc** |**Sens**|**Prec**|**Acc** |**Sens**|**Prec**|
|**Iris-setosa**|100|100|100|100|100|100|
|**Iris-versicolor**|97.78|100|92.86|95.33|93.94|91.91|
|**Iris-virginica**|97.78|92.31|100|95.33|92.59|93.86|
|**\*Mean**|98.52|97.44|97.62|96.89|95.51|95.26|
> The row **\*Mean** is the mean of three label being positive

## Comparison & Conclusion
### Mushoroom
**1. Laplace smoothing ä½¿çµæœè®Šå·®**

æ ¹æ“šä¸Šæ–¹çš„ resultï¼Œå¯ä»¥ç™¼ç¾æœªåŠ ä¸Š Laplace smoothing çš„çµæœè¼ƒå¥½ï¼Œæº–ç¢ºç‡ (Accuracy) é” 99.63%ï¼Œä½†åŠ ä¸Š Laplace smoothing ä¹‹å¾Œçµæœå»è®Šå·®ï¼Œæº–ç¢ºç‡ (Accuracy) åªæœ‰ 95.41%ï¼Œä¸è«–ä½¿ç”¨ Holdout é‚„æ˜¯ K-fold validation éƒ½æœ‰é€™ç¨®æƒ…æ³ã€‚

### Iris

## Question
### Mushoroom
```python
cate_list = ['n','b','c','g','o','p','e','w','y'] # All categories that 'stalk-color-below-ring' has
num_Xi_y = [0,0,0,0,0,0,0,0,0]

filter_e = (data['edible']=='e')
num_y = len(data[filter_e])
cnt = data[filter_e]['stalk-color-below-ring'].value_counts()
num_Xi_y[7] = cnt[0]
num_Xi_y[3] = cnt[1]
num_Xi_y[5] = cnt[2]
num_Xi_y[4] = cnt[3]
num_Xi_y[6] = cnt[4]
num_Xi_y[0] = cnt[5]

# Without Laplace
prob = [x / num_y for x in num_Xi_y]

# With Laplace
alpha = 0.1
prob_l = [(x+alpha) / num_y+(alpha*len(cate_list)) for x in num_Xi_y]
```
> Laplace alpha = 0.1

![Question_mushroom](https://raw.githubusercontent.com/denny3388/ML_HW1/master/pictures/Question_mushroom.png)

### Iris

æ ¹æ“š Data visualizationï¼Œä¸‹åœ–ç‚ºåœ¨ class = Iris-Versicolour æ™‚ï¼Œpetal-length çš„åˆ†å¸ƒ

![Question_Iris_1](https://raw.githubusercontent.com/denny3388/ML_HW1/master/pictures/Question_Iris_1.png)

æŒ‰ç…§åˆ†å¸ƒæƒ…æ³ï¼Œæ­¤è™•æˆ‘å€‘é¸ç”¨ **Normal distribution** ä¾† fit é€™å€‹æ©Ÿç‡

---> ğ‘ƒ(ğ‘‹ğ‘ğ‘’ğ‘¡ğ‘ğ‘™_ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„|ğ‘Œ=Iris Versicolour) â‰’ *N(X, Î¼, Ïƒ)*

- **mean** of *P()* = mean of *N()* = Î¼ = sample mean = **4.26**
- **S.D.** of *P()* = S.D. of *N()* = Ïƒ = sample S.D. = **0.469911**
- ä»¥ä¸‹ç‚º **PDF** of ğ‘ƒ(ğ‘‹ğ‘ğ‘’ğ‘¡ğ‘ğ‘™_ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„|ğ‘Œ=Iris Versicolour)

![Question_Iris_2](https://raw.githubusercontent.com/denny3388/ML_HW1/master/pictures/Question_Iris_2.png)
s